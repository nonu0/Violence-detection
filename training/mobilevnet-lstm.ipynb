{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4718786,"sourceType":"datasetVersion","datasetId":2730182}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torchvision.models as models\nimport torch.nn as nn\nimport numpy as np\nimport torch\nvideo_path = r'/kaggle/input/rwf2000/RWF-2000/val/Fight/48J5lk4QcpE_3.avi'\nroot_dir = r'/kaggle/input/rwf2000/RWF-2000'\ntrain_data_root = r'/kaggle/input/rwf2000/RWF-2000/train'\nval_data_root = r'/kaggle/input/rwf2000/RWF-2000/val'\n# video_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:20:31.403128Z","iopub.execute_input":"2025-06-06T16:20:31.403391Z","iopub.status.idle":"2025-06-06T16:20:31.407787Z","shell.execute_reply.started":"2025-06-06T16:20:31.403372Z","shell.execute_reply":"2025-06-06T16:20:31.406894Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from torchvision.models import MobileNet_V2_Weights\n\nweights = MobileNet_V2_Weights.DEFAULT\ntransform = weights.transforms()\n\n# Shows all preprocessing steps including expected shape\nprint(transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:20:32.493597Z","iopub.execute_input":"2025-06-06T16:20:32.494300Z","iopub.status.idle":"2025-06-06T16:20:32.498564Z","shell.execute_reply.started":"2025-06-06T16:20:32.494273Z","shell.execute_reply":"2025-06-06T16:20:32.497866Z"}},"outputs":[{"name":"stdout","text":"ImageClassification(\n    crop_size=[224]\n    resize_size=[232]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass MobileNetLstmModel(nn.Module):\n    def __init__(self,hidden_state=512, num_classes=2, lstm_layers=1):\n        super(MobileNetLstmModel,self).__init__()\n        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n        self.feature_extractor = mobilenet.features \n        self.pool = nn.AdaptiveAvgPool2d((1,1,))\n        self.lstm = nn.LSTM(input_size=1280,\n                            hidden_size = hidden_state,\n                            num_layers = lstm_layers,\n                            batch_first=True,\n                            bidirectional = True                \n                            )\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_state*2,128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128,num_classes),\n        )\n\n    def forward(self,x):\n        B,T,C,H,W = x.shape\n        x = x.view(B*T,C,H,W)\n        x = self.feature_extractor(x)\n        # print(self.feature_extractor)\n        x = self.pool(x).squeeze(-1).squeeze(-1)\n        x = x.view(B,T,-1)\n        output,(hn,cn) = self.lstm(x)\n        out = output[:,-1,:]\n        # print(out.shape)\n        final_out = self.classifier(out)\n        # print(final_out)\n        # print(final_out.shape)\n        return final_out\n\n# data = torch.randn([4,16,3,224,224])\n# model = MobileNetLstmModel()\n# print(model(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:20:34.657809Z","iopub.execute_input":"2025-06-06T16:20:34.658522Z","iopub.status.idle":"2025-06-06T16:20:34.665170Z","shell.execute_reply.started":"2025-06-06T16:20:34.658497Z","shell.execute_reply":"2025-06-06T16:20:34.664369Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport random\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nclass ViolenceDataset(Dataset):\n    def __init__(self, root_dir, clip_len=16, transform=None):\n        self.clip_len = clip_len\n        self.transform = transform\n\n        self.data = []\n        self.labels = []\n\n        class_map = {\"NonFight\": 0, \"Fight\": 1}\n        for label_name, label_val in class_map.items():\n            class_dir = os.path.join(root_dir, label_name)\n            for fname in os.listdir(class_dir):\n                if fname.endswith(\".mp4\") or fname.endswith(\".avi\"):\n                    self.data.append(os.path.join(class_dir, fname))\n                    self.labels.append(label_val)\n\n    def read_video(self, path):\n        cap = cv2.VideoCapture(path)\n        frames = []\n\n        frame_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        if frame_total == 0:\n            cap.release()\n            return [np.zeros((224, 224, 3), dtype=np.uint8)] * self.clip_len\n\n        if frame_total >= self.clip_len:\n            frame_idx = np.linspace(0, frame_total - 1, self.clip_len, dtype=np.int32)\n        else:\n            frame_idx = np.linspace(0, frame_total - 1, frame_total, dtype=np.int32)\n\n        for idx in frame_idx:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.resize(frame, (224, 224))\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frames.append(frame)\n            else:\n                frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # fallback frame\n\n        cap.release()\n\n        # Pad if too short\n        if len(frames) < self.clip_len:\n            frames += [frames[-1]] * (self.clip_len - len(frames))\n\n        return frames\n\n    def __getitem__(self, index):\n        video_path = self.data[index]\n        label = self.labels[index]\n\n        frames = self.read_video(video_path)\n\n        if self.transform:\n            frames = [self.transform(frame) for frame in frames]\n\n        clip = torch.stack(frames)  # (T, C, H, W)\n        # clip = clip.permute(1,0,2,3) # (C,T,H,W)\n        return clip, torch.tensor(label, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:20:34.951522Z","iopub.execute_input":"2025-06-06T16:20:34.952304Z","iopub.status.idle":"2025-06-06T16:20:35.230102Z","shell.execute_reply.started":"2025-06-06T16:20:34.952273Z","shell.execute_reply":"2025-06-06T16:20:35.229511Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from torchvision import transforms\nimport torch\nfrom torchvision.transforms import InterpolationMode\n\nr3d_transform = transforms.Compose([\n    transforms.ToPILImage(), \n    transforms.Resize([232, 232], interpolation=InterpolationMode.BILINEAR),\n    transforms.CenterCrop([224, 224]),\n    transforms.ToTensor(), \n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:20:35.231314Z","iopub.execute_input":"2025-06-06T16:20:35.231568Z","iopub.status.idle":"2025-06-06T16:20:35.235920Z","shell.execute_reply.started":"2025-06-06T16:20:35.231546Z","shell.execute_reply":"2025-06-06T16:20:35.235220Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_dataset = ViolenceDataset(train_data_root,clip_len=16,transform=r3d_transform)\nval_dataset = ViolenceDataset(val_data_root,clip_len=16,transform=r3d_transform)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:20:40.874692Z","iopub.execute_input":"2025-06-06T16:20:40.875214Z","iopub.status.idle":"2025-06-06T16:20:41.261069Z","shell.execute_reply.started":"2025-06-06T16:20:40.875188Z","shell.execute_reply":"2025-06-06T16:20:41.260356Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\nval_loader = DataLoader(val_dataset,batch_size=4,shuffle=False)\n# for i,l in train_loader:\n#     print(i.size())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:20:41.262177Z","iopub.execute_input":"2025-06-06T16:20:41.262421Z","iopub.status.idle":"2025-06-06T16:20:41.266537Z","shell.execute_reply.started":"2025-06-06T16:20:41.262404Z","shell.execute_reply":"2025-06-06T16:20:41.265788Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# !pip install mlflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:20:44.653983Z","iopub.execute_input":"2025-06-06T16:20:44.654710Z","iopub.status.idle":"2025-06-06T16:20:44.658177Z","shell.execute_reply.started":"2025-06-06T16:20:44.654678Z","shell.execute_reply":"2025-06-06T16:20:44.657374Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport mlflow\nimport mlflow.pytorch\nimport os\n\n# Assuming model, train_loader, val_loader are already defined\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MobileNetLstmModel().to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n# Setup\nbest_val_acc = 0.0\nnum_epochs = 20\nrun_name = \"mobilenet_lstm_run\"\n\n# Directory to save best model\nbest_model_path = \"best_model.pt\"\n\n# MLflow tracking\nmlflow.set_experiment(\"MobileNetLSTM Video Classification\")\n\nwith mlflow.start_run(run_name=run_name):\n    mlflow.log_params({\n        \"model\": \"MobileNetLSTM\",\n        \"optimizer\": \"Adam\",\n        \"lr\": 1e-4,\n        \"loss\": \"CrossEntropyLoss\",\n        \"epochs\": num_epochs,\n        \"scheduler\": \"StepLR(5, gamma=0.5)\"\n    })\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        train_correct, train_total = 0, 0\n\n        for clips, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n            clips = clips.to(device)\n            labels = labels.to(device)\n            # print(clips.shape)\n            optimizer.zero_grad()\n            outputs = model(clips)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            # print(outputs)\n\n            train_loss += loss.item() * clips.size(0)\n            _, predicted = torch.max(outputs, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n\n        train_acc = 100.0 * train_correct / train_total\n        train_loss /= train_total\n\n        # ---------------- Validation ---------------- #\n        model.eval()\n        val_loss = 0.0\n        val_correct, val_total = 0, 0\n\n        with torch.no_grad():\n            for clips, labels in tqdm(val_loader, desc=\"Validating\"):\n                clips = clips.to(device)\n                labels = labels.to(device)\n\n                outputs = model(clips)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item() * clips.size(0)\n                _, predicted = torch.max(outputs, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        val_acc = 100.0 * val_correct / val_total\n        val_loss /= val_total\n\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.2f}%\")\n\n        # Log to MLflow\n        mlflow.log_metrics({\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc,\n            \"lr\": optimizer.param_groups[0]['lr']\n        }, step=epoch)\n\n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), best_model_path)\n            mlflow.log_artifact(best_model_path)\n            print(f\"✅ Saved Best Model @ Epoch {epoch+1} with Val Acc: {val_acc:.2f}%\")\n\n        scheduler.step()\n\n    # Log final model\n    mlflow.pytorch.log_model(model, \"final_model\")\n    mlflow.log_metric(\"best_val_accuracy\", best_val_acc)\n    print(\"🚀 Training complete. Best Val Acc:\", best_val_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:20:45.026270Z","iopub.execute_input":"2025-06-06T16:20:45.026487Z","iopub.status.idle":"2025-06-06T20:44:08.250377Z","shell.execute_reply.started":"2025-06-06T16:20:45.026469Z","shell.execute_reply":"2025-06-06T20:44:08.249665Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 125MB/s]\n2025/06/06 16:20:45 INFO mlflow.tracking.fluent: Experiment with name 'MobileNetLSTM Video Classification' does not exist. Creating a new experiment.\nTraining Epoch 1/20: 100%|██████████| 400/400 [11:38<00:00,  1.75s/it]\nValidating: 100%|██████████| 100/100 [02:48<00:00,  1.69s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 0.5870, Train Acc: 69.25%\nVal   Loss: 0.5648, Val   Acc: 71.50%\n✅ Saved Best Model @ Epoch 1 with Val Acc: 71.50%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2/20: 100%|██████████| 400/400 [10:37<00:00,  1.59s/it]\nValidating: 100%|██████████| 100/100 [02:31<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20\nTrain Loss: 0.4641, Train Acc: 78.56%\nVal   Loss: 0.4580, Val   Acc: 76.50%\n✅ Saved Best Model @ Epoch 2 with Val Acc: 76.50%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3/20: 100%|██████████| 400/400 [10:34<00:00,  1.59s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20\nTrain Loss: 0.3826, Train Acc: 82.69%\nVal   Loss: 0.4757, Val   Acc: 77.50%\n✅ Saved Best Model @ Epoch 3 with Val Acc: 77.50%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4/20: 100%|██████████| 400/400 [10:32<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.50s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20\nTrain Loss: 0.2991, Train Acc: 87.69%\nVal   Loss: 0.7797, Val   Acc: 68.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5/20: 100%|██████████| 400/400 [10:33<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20\nTrain Loss: 0.2183, Train Acc: 91.06%\nVal   Loss: 0.6320, Val   Acc: 74.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6/20: 100%|██████████| 400/400 [10:33<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20\nTrain Loss: 0.1605, Train Acc: 94.50%\nVal   Loss: 0.7535, Val   Acc: 73.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7/20: 100%|██████████| 400/400 [10:33<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20\nTrain Loss: 0.1059, Train Acc: 96.69%\nVal   Loss: 0.8255, Val   Acc: 71.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8/20: 100%|██████████| 400/400 [10:34<00:00,  1.59s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20\nTrain Loss: 0.0707, Train Acc: 97.56%\nVal   Loss: 0.7819, Val   Acc: 77.50%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9/20: 100%|██████████| 400/400 [10:35<00:00,  1.59s/it]\nValidating: 100%|██████████| 100/100 [02:31<00:00,  1.52s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20\nTrain Loss: 0.0632, Train Acc: 97.88%\nVal   Loss: 0.9835, Val   Acc: 73.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10/20: 100%|██████████| 400/400 [10:36<00:00,  1.59s/it]\nValidating: 100%|██████████| 100/100 [02:31<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20\nTrain Loss: 0.0597, Train Acc: 97.81%\nVal   Loss: 1.0035, Val   Acc: 71.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 11/20: 100%|██████████| 400/400 [10:35<00:00,  1.59s/it]\nValidating: 100%|██████████| 100/100 [02:31<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20\nTrain Loss: 0.0359, Train Acc: 98.69%\nVal   Loss: 1.0367, Val   Acc: 73.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 12/20: 100%|██████████| 400/400 [10:36<00:00,  1.59s/it]\nValidating: 100%|██████████| 100/100 [02:31<00:00,  1.52s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20\nTrain Loss: 0.0341, Train Acc: 98.56%\nVal   Loss: 1.0930, Val   Acc: 73.25%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 13/20: 100%|██████████| 400/400 [10:35<00:00,  1.59s/it]\nValidating: 100%|██████████| 100/100 [02:31<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20\nTrain Loss: 0.0344, Train Acc: 98.62%\nVal   Loss: 1.0090, Val   Acc: 75.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 14/20: 100%|██████████| 400/400 [10:32<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20\nTrain Loss: 0.0306, Train Acc: 99.12%\nVal   Loss: 1.0812, Val   Acc: 74.25%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 15/20: 100%|██████████| 400/400 [10:34<00:00,  1.59s/it]\nValidating: 100%|██████████| 100/100 [02:31<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20\nTrain Loss: 0.0283, Train Acc: 99.00%\nVal   Loss: 1.2694, Val   Acc: 72.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 16/20: 100%|██████████| 400/400 [10:32<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20\nTrain Loss: 0.0210, Train Acc: 99.38%\nVal   Loss: 1.0749, Val   Acc: 74.25%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 17/20: 100%|██████████| 400/400 [10:33<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20\nTrain Loss: 0.0122, Train Acc: 99.50%\nVal   Loss: 1.2037, Val   Acc: 74.50%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 18/20: 100%|██████████| 400/400 [10:33<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:31<00:00,  1.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20\nTrain Loss: 0.0132, Train Acc: 99.69%\nVal   Loss: 1.2142, Val   Acc: 74.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 19/20: 100%|██████████| 400/400 [10:33<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.50s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20\nTrain Loss: 0.0143, Train Acc: 99.50%\nVal   Loss: 1.2067, Val   Acc: 71.75%\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 20/20: 100%|██████████| 400/400 [10:32<00:00,  1.58s/it]\nValidating: 100%|██████████| 100/100 [02:30<00:00,  1.50s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20\nTrain Loss: 0.0114, Train Acc: 99.56%\nVal   Loss: 1.1316, Val   Acc: 75.25%\n","output_type":"stream"},{"name":"stderr","text":"2025/06/06 20:43:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/06/06 20:44:08 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.21.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torchvision==0.21.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\u001b[31m2025/06/06 20:44:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"🚀 Training complete. Best Val Acc: 77.5\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('mlruns_backup', 'zip', 'mlruns')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T21:01:26.533117Z","iopub.execute_input":"2025-06-06T21:01:26.533773Z","iopub.status.idle":"2025-06-06T21:01:30.325221Z","shell.execute_reply.started":"2025-06-06T21:01:26.533748Z","shell.execute_reply":"2025-06-06T21:01:30.324462Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/mlruns_backup.zip'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"frames = []\nvideo_path = r'/kaggle/input/rwf2000/RWF-2000/val/Fight/48J5lk4QcpE_3.avi'\ncap = cv2.VideoCapture(video_path)\nwhile True:\n    ret,frame = cap.read()\n    if not ret:\n        break\n    frame = cv2.resize(frame,(224,224))\n    frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n    frames.append(frame)\n    # print(len(frames))\ncap.release()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:34:29.476781Z","iopub.execute_input":"2025-06-05T08:34:29.477145Z","iopub.status.idle":"2025-06-05T08:34:30.160233Z","shell.execute_reply.started":"2025-06-05T08:34:29.477115Z","shell.execute_reply":"2025-06-05T08:34:30.159408Z"}},"outputs":[],"execution_count":226},{"cell_type":"code","source":"import random\n\n# if len(frames) < 16:\n#     frames = frames + [frames[-1]] * (16 - len(frames))  # pad last frame\nstart = random.randint(0,len(frames)-16)\n# clip = \nclip = frames[start:start+16]\nlen(clip)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T07:39:18.508419Z","iopub.execute_input":"2025-06-05T07:39:18.508739Z","iopub.status.idle":"2025-06-05T07:39:18.515619Z","shell.execute_reply.started":"2025-06-05T07:39:18.508715Z","shell.execute_reply":"2025-06-05T07:39:18.514427Z"}},"outputs":[{"execution_count":159,"output_type":"execute_result","data":{"text/plain":"16"},"metadata":{}}],"execution_count":159},{"cell_type":"code","source":"len(r'/kaggle/input/rwf2000/RWF-2000/val/NonFight')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T07:37:23.182759Z","iopub.execute_input":"2025-06-05T07:37:23.183139Z","iopub.status.idle":"2025-06-05T07:37:23.189902Z","shell.execute_reply.started":"2025-06-05T07:37:23.183111Z","shell.execute_reply":"2025-06-05T07:37:23.188549Z"}},"outputs":[{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"43"},"metadata":{}}],"execution_count":145},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}